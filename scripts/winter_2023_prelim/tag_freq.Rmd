---
title: "Tag Frequency Changes"
author: "Janette Avelar"
date: '2023-12-16'
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)
pacman::p_load(tidyverse, here, rio, DT, ggcorrplot, psych, parameters)
#read in data
dat <- import(here("data/longitudinal", "tags-long.csv"))
#read in tag labels
labels <- import(here("data", "tag_labels.csv"))
```

*Central Q: * For each practice where we have at least 2 years of data, how has the practice's frequency changed?  
-look only at changes within schools - so, schools that have at least 2 data points about that practice
-the object of inquiry here is the practice - how have each practice's frequencies changed?

A few ideas:  
-could be a table showing +/-X (where X is number of schools adding or removing) for each year. We did a graph version of this on p.8 in the 2021 report.  
-could be a line graph showing # of schools reporting each practice each year, similar to p.22 in the 2022 report. (Note, this code should be in our github from 2022.)  
-to make it more readable, it could be interactive (user could hover over a tag to see that line distinguished).  

### Raw Counts Table

```{r}
##### creating table with raw count differences across years #####
#starting with raw counts for each year
dat %>% 
  group_by(year, var) %>% 
  summarize(n = sum(usage, na.rm = TRUE)) %>% 
  #need to also figure out how many repeat over year
  ungroup() %>% 
  mutate(rate = rep(1, nrow(.))) %>% 
  group_by(var) %>% 
  mutate(rep = sum(rate, na.rm = TRUE)) %>% 
  ungroup() %>% 
  #drop vars without 2+ years of data
  filter(rep > 1) %>% 
  select(!c(rate, rep)) %>% 
  #make cols for each year to determine change
  pivot_wider(names_from = year,
              values_from = n) %>% 
  #calculate change
  mutate(change_2021 = `2021` - `2019`,
         change_2022 = `2022` - `2021`,
         change_2023 = `2023` - `2022`) %>% 
  #bring in labels
  left_join(labels, by = "var") %>% 
  select(Tag = label, `2019`, `2021` = change_2021, `2022` = change_2022, `2023` = change_2023) %>% 
  #table
  datatable() %>%
  #style
  formatStyle(columns = c("2021", "2022", "2023"), 
              color = styleInterval(cuts = 0, values = c("red", "green")),
              textAlign = "center")
```

### Percentage Table

```{r}
##### creating table with percentage differences across years #####
#year totals
totals <- dat %>% 
  select(school_id, year) %>% 
  unique() %>% 
  mutate(rate = rep(1, nrow(.))) %>% 
  group_by(year) %>% 
  summarize(n_school = sum(rate, na.rm = TRUE)) %>% 
  ungroup()
# create table with percentage changes
dat %>% 
  group_by(year, var) %>% 
  summarize(n = sum(usage, na.rm = TRUE)) %>% 
  left_join(totals, by = "year") %>% 
  mutate(pct = n/n_school,
         rate = rep(1, n())) %>% 
  ungroup() %>% 
  #need to also figure out how many repeat over year
  group_by(var) %>% 
  mutate(rep = sum(rate, na.rm = TRUE)) %>% 
  ungroup() %>% 
  #drop vars without 2+ years of data
  filter(rep > 1) %>% 
  select(var, year, pct) %>% 
  #make cols for each year
  pivot_wider(names_from = year,
              values_from = pct) %>%
  #calculate change
  mutate(change_2021 = `2021` - `2019`,
         change_2022 = `2022` - `2021`,
         change_2023 = `2023` - `2022`) %>% 
  #bring in labels
  left_join(labels, by = "var") %>% 
  select(Tag = label, `2019`, `2021` = change_2021, `2022` = change_2022, `2023` = change_2023) %>% 
  #table
  datatable() %>%
  #style
  formatStyle(columns = c("2021", "2022", "2023"), 
              color = styleInterval(cuts = 0, values = c("red", "green")),
              textAlign = "center") %>% 
  formatPercentage(c("2019", "2021", "2022", "2023"), 0)
```

### Graph by Cluster

Focusing in on tags with complete data - correlations are thrown off by missing data. Suggestions for resolving?

```{r, include = FALSE}
# recreate EFA to cluster across years
#import wide file & filter to complete obs
complete <- dat %>% 
  select(var, year) %>% 
  unique() %>% 
  mutate(rate = rep(1, nrow(.))) %>% 
  group_by(var) %>% 
  summarize(n = sum(rate)) %>% 
  filter(n == 4) %>% 
  pull(var)
  
wide <- import(here("data/longitudinal", "tags-wide.csv")) %>% 
  select(school_id, year, all_of(complete))
#generate correlations
cor <- wide %>%
  select(starts_with("practices")) %>%
  cor
#quick cor plot
plot_cor <- function(cor_mat, title = "") {
  ggcorrplot(cor_mat, hc.order = T, type = "upper") +
    scale_fill_distiller(type = "div", limits = c(-1, 1), expand = c(0, 0)) +
    labs(title = title,
         fill = "Correlation") +
    scale_x_discrete(labels = labels) +
    scale_y_discrete(labels = labels) +
    labs(x = "", y = "") + 
    # theme_transcend_sparse + 
    theme(axis.text.x = element_blank(), 
          axis.text.y = element_blank(),
          panel.border = element_blank(),
          axis.ticks = element_blank()
          )
}

plot_cor(cor, 
         title = "Correlation heat map for all tags across years") +
theme(legend.position = c(.85, .25))
#prep df
mat_to_df <- function(m) {
  data.frame(row=rownames(m)[row(m)[upper.tri(m)]], 
           col=colnames(m)[col(m)[upper.tri(m)]], 
           corr=m[upper.tri(m)])
}

df_cor <- mat_to_df(cor)
#scree
fa_cor <- fa.parallel(
  cor, fm = "pa", fa = "fa", n.obs = nrow(wide),
  main = "Correlation scree plot"
) #suggested number of factors = 9
#testing out 4
fa(cor, nfactors = 4, rotate = "oblimin", fm = "minres") %>% 
  model_parameters(sort = TRUE, threshold = "max") %>%
  datatable(caption = "Max loadings") %>%
  formatRound(digits = 2, columns = 2:6)
```

